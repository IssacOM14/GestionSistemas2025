## Key Features of NeuroChat â­

- ðŸš€ **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.

- ðŸ¤ **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.

- ðŸ“± **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.

- ðŸ“± **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.

- âœ’ï¸ðŸ”¢ **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.

- ðŸŽ¤ðŸ“¹ **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.

- ðŸ **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.

- ðŸ“š **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the `#` command before a query.

- ðŸ” **Web Search for RAG**: Perform web searches using providers like `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo` and `TavilySearch` and inject the results directly into your chat experience.

- ðŸŒ **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.

- ðŸŽ¨ **Image Generation Integration**: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.

- âš™ï¸ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.

- ðŸ” **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.

- ðŸŒðŸŒ **Multilingual Support**: Experience NeuroChat in your preferred language with our internationalization (i18n) support.
## How to Install ðŸš€

### Minimum system requirements for its execution:

1. 8GB of VRAM
2. 12GB of RAM
3. CPU with 6 cores
4. 25GB of storage (depending on the models we want to use)

### Quick Start with Docker ðŸ³

> [!TIP]  
> If you wish to utilize NeuroChat with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.

### Installation with Default Configuration

- **If Ollama is on your computer** :
  Create the Docker Image. Then, save that image and execute it, make sure that you have Docker Desktop running. Notice that you can create the image depending on the system requirement (like CUDA).

### Installation for OpenAI API Usage Only

- **If you're only using OpenAI API** :
  Make sure that you have the token, then go to the backend folder and add that token. (Make sure to run this on a trusted network.

#### NeuroChat: Server Connection Error

If you're experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.

### Installation via Terminal
Make sure that you are running on a node version 18 or above. On the other hand, you will need an environment (like conda)

1. Clone the repo.
2. Create the conda env with python v3.11 or above.
3. Activate the env and go to the backend folder, and install the requirements.txt with pip.
4. On another terminal, go to the main directory of the project. In this case, you will not need a conda env. Just run the "npm install" command. Finally, run the "build" script.
5. Now, we run the "bash start.sh" on the conda env.

   AND THAT'S IT!


### Acknowledgments

Special thanks to [Prof. Lawrence Kim](https://www.lhkim.com/) and [Prof. Nick Vincent](https://www.nickmvincent.com/) for their invaluable support and guidance in shaping this project into a research endeavor.
